{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Exercise 4 - Optimizing Model Training\n\nIn [the previous exercise](./03%20-%20Compute%20Contexts.ipynb), you created cloud-based compute and used it when running a model training experiment. The benefit of cloud compute is that it offers a cost-effective way to scale out your experiment workflow and try different algorithms and parameters in order to optimize your model's performance; and that's what we'll explore in this exercise.\n\n> **Important**: This exercise assumes you have completed the previous exercises in this series - specifically, you must have:\n>\n> - Created an Azure ML Workspace.\n> - Uploaded the diabetes.csv data file to the workspace's default datastore.\n> - Registered a **Diabetes Dataset** dataset in the workspace.\n> - Provisioned an Azure ML Compute resource named **cpu-cluster**.\n>\n> If you haven't done that, now would be a good time - nobody's going to do it for you!\n\n## Task 1: Connect to Your Workspace\n\nThe first thing you need to do is to connect to your workspace using the Azure ML SDK. Let's start by ensuring you still have the latest version installed (if you ended and restarted your Azure Notebooks session, the environment may have been reset)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade azureml-sdk[notebooks,automl,explain]\n\nimport azureml.core\nprint(\"Ready to use Azure ML\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you're ready to connect to your workspace. When you created it in the previous exercise, you saved its configuration; so now you can simply load the workspace from its configuration file.\n\n> **Note**: If the authenticated session with your Azure subscription has expired since you completed the previous exercise, you'll be prompted to reauthenticate."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Load the workspace from the saved config file\nws = Workspace.from_config()\nprint('Ready to work with', ws.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's get the Azure ML compute resource you created previously (or recreate it if you deleted it!)"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# Choose a name for your CPU cluster\ncpu_cluster_name = \"cpu-cluster\"\n\n# Verify that cluster does not exist already\ntry:\n    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n    print('Found existing cluster, use it.')\nexcept ComputeTargetException:\n    # Create an AzureMl Compute resource (a container cluster)\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n                                                           vm_priority='lowpriority', \n                                                           max_nodes=4)\n    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n\ncpu_cluster.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task 2: Use *Hyperdrive* to Determine Optimal Parameter Values\n\nThe remote compute you created is a four-node cluster, and you can take advantage of this to execute multiple experiment runs in parallel. One key reason to do this is to try training a model with a range of different hyperparameter values.\n\nAzure ML includes a feature called *hyperdrive* that enables you to randomly try different values for one or more hyperparameters, and find the best performing trained model based on a metric that you specify - such as *Accuracy* or *Area Under the Curve (AUC)*.\n\n> **More Information**: For more information about Hyperdrive, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters).\n\nLet's run a Hyperdrive experiment on the remote compute you have provisioned. First, we'll create the experiment and its associated folder."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nfrom azureml.core import Experiment\n\n# Create an experiment\nexperiment_name = 'diabetes_training'\nexperiment = Experiment(workspace = ws, name = experiment_name)\n\n# Create a folder for the experiment files\nexperiment_folder = './' + experiment_name\nos.makedirs(experiment_folder, exist_ok=True)\n\nprint(\"Experiment:\", experiment.name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we'll create the Python script our experiment will run in order to train a model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $experiment_folder/diabetes_training.py\n# Import libraries\nimport argparse\nimport joblib\nfrom azureml.core import Workspace, Dataset, Experiment, Run\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\n# Set regularization parameter\nparser = argparse.ArgumentParser()\nparser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\nargs = parser.parse_args()\nreg = args.reg_rate\n\n# Get the experiment run context\nrun = Run.get_context()\n\n# load the diabetes dataset\ndataset_name = 'Diabetes Dataset'\nprint(\"Loading data from \" + dataset_name)\ndiabetes = Dataset.get_by_name(workspace=run.experiment.workspace, name=dataset_name).to_pandas_dataframe()\n\n# Separate features and labels\nX, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n\n# Split data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\n# Train a logistic regression model\nprint('Training a logistic regression model with regularization rate of', reg)\nrun.log('Regularization Rate',  np.float(reg))\nmodel = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n\n# calculate accuracy\ny_hat = model.predict(X_test)\nacc = np.average(y_hat == y_test)\nprint('Accuracy:', acc)\nrun.log('Accuracy', np.float(acc))\n\n# calculate AUC\ny_scores = model.predict_proba(X_test)\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint('AUC: ' + str(auc))\nrun.log('AUC', np.float(auc))\n\n# plot ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\nfig = plt.figure(figsize=(6, 4))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nrun.log_image(name = \"ROC\", plot = fig)\nplt.show()\n\nos.makedirs('outputs', exist_ok=True)\n# note file saved in the outputs folder is automatically uploaded into experiment record\njoblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n\nrun.complete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, we'll use the *Hyperdrive* feature of Azure ML to run multiple experiments in parallel, using different values for the **regularization** parameter to find the optimal value for our data."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.hyperdrive import GridParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\nfrom azureml.train.hyperdrive import choice\nfrom azureml.widgets import RunDetails\nfrom azureml.train.sklearn import SKLearn\n\n# Sample a range of parameter values\nparams = GridParameterSampling(\n    {\n        # There's only one parameter, so grid sampling will try each value - with multiple parameters it would try every combination\n        '--regularization': choice(0.001, 0.005, 0.01, 0.05, 0.1, 1.0)\n    }\n)\n\n# Set evaluation policy to stop poorly performing training runs early\npolicy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n\n# Create an estimator that uses the remote compute\nhyper_estimator = SKLearn(source_directory=experiment_folder,\n                           compute_target = cpu_cluster,\n                           conda_packages=['pandas','ipykernel','matplotlib'],\n                           pip_packages=['azureml-sdk','argparse','pyarrow'],\n                           entry_script='diabetes_training.py')\n\n# Configure hyperdrive settings\nhyperdrive = HyperDriveConfig(estimator=hyper_estimator, \n                          hyperparameter_sampling=params, \n                          policy=policy, \n                          primary_metric_name='AUC', \n                          primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                          max_total_runs=6,\n                          max_concurrent_runs=4)\n\n\n# Run the experiment\nrun = experiment.submit(config=hyperdrive)\n\n# Show the status in the notebook as the experiment runs\nRunDetails(run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "When all of the runs have finished, you can find the best one based on the performance metric you specified (in this case, the one with the best AUC)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run = run.get_best_run_by_primary_metric()\nbest_run_metrics = best_run.get_metrics()\nparameter_values = best_run.get_details() ['runDefinition']['arguments']\n\nprint('Best Run Id: ', best_run.id)\nprint(' -AUC:', best_run_metrics['AUC'])\nprint(' -Accuracy:', best_run_metrics['Accuracy'])\nprint(' -Regularization Rate:',parameter_values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Since we've found the best run, we can register the model it trained."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Model\n\n# Register model\nbest_run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model', tags={'Training context':'Hyperdrive'}, properties={'AUC': best_run_metrics['AUC'], 'Accuracy': best_run_metrics['Accuracy']})\n\n# List registered models\nfor model in Model.list(ws):\n    print(model.name, 'version:', model.version)\n    for tag_name in model.tags:\n        tag = model.tags[tag_name]\n        print ('\\t',tag_name, ':', tag)\n    for prop_name in model.properties:\n        prop = model.properties[prop_name]\n        print ('\\t',prop_name, ':', prop)\n    print('\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Task 3: Use *Auto ML* to Find the Best Model\n\nHyperparameter tuning has helped us find the optimal regularization rate for our logistic regression model, but we might get better results by trying a different algorithm, and by performing some basic feature-engineering, such as scaling numeric feature values. You could just create lots of different training scripts that apply various scikit-learn algorithms, and try them all until you find the best result; but Azure ML provides a feature called *Automated Machine Learning* (or *Auto ML*) that can do this for you.\n\nFirst, let's create a folder for a new experiment."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create a project folder if it doesn't exist\nautoml_folder = \"automl_experiment\"\nif not os.path.exists(automl_folder):\n    os.makedirs(automl_folder)\nprint(automl_folder, 'folder created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You don't need to create a training script (Auto ML will do that for you), but you do need to load the training data; and when using remote compute, this is best achieved by creating a script containing a **get_data** function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $automl_folder/get_data.py\n#Write the get_data file.\nfrom azureml.core import Run, Workspace, Dataset\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndef get_data():\n\n    # load the diabetes dataset\n    run = Run.get_context()\n    dataset_name = 'Diabetes Dataset'\n    diabetes = Dataset.get_by_name(workspace=run.experiment.workspace, name=dataset_name).to_pandas_dataframe()\n\n    # Separate features and labels\n    X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n    \n    # Split data into training set and test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\n    return { \"X\" : X_train, \"y\" : y_train, \"X_valid\" : X_test, \"y_valid\" : y_test }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you're ready to configure the Auto ML experiment. To do this, you'll need a run configuration that includes the required packages for the experiment environment, and a set of configuration settings that tells Auto ML how many options to try, which metric to use when evaluating models, and so on.\n\n> **More Information**: For more information about options when using Auto ML, see the [Azure ML documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.train.automl import AutoMLConfig\nimport time\nimport logging\n\n\nautoml_run_config = RunConfiguration(framework=\"python\")\nautoml_run_config.environment.docker.enabled = True\n\nauto_ml_dependencies = CondaDependencies.create(\n    pip_packages=[\"azureml-sdk\", \"pyarrow\", \"pandas\", \"scikit-learn\", \"numpy\"])\nautoml_run_config.environment.python.conda_dependencies = auto_ml_dependencies\n\n\nautoml_settings = {\n    \"name\": \"Diabetes_AutoML_{0}\".format(time.time()),\n    \"iteration_timeout_minutes\": 10,\n    \"iterations\": 10,\n    \"primary_metric\": 'AUC_weighted',\n    \"preprocess\": False,\n    \"max_concurrent_iterations\": 4,\n    \"verbosity\": logging.INFO\n}\n\nautoml_config = AutoMLConfig(task='classification',\n                             debug_log='automl_errors.log',\n                             path=automl_folder,\n                             compute_target=cpu_cluster,\n                             run_configuration=automl_run_config,\n                             data_script=automl_folder + \"/get_data.py\",\n                             model_explainability=True,\n                             **automl_settings,\n                             )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "OK, we're ready to go. Let's start the Auto ML run, which will generate child runs for different algorithms.\n\n> **Note**: This will take some time. Progress will be displayed as each child run completes, and then a widget showing the results will be displayed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.experiment import Experiment\nfrom azureml.widgets import RunDetails\n\nautoml_experiment = Experiment(ws, 'diabetes_automl')\nautoml_run = automl_experiment.submit(automl_config, show_output=True)\nRunDetails(automl_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "View the output of the experiment in the widget, and click the run that produced the best result to see its details.\nThen click the link to view the experiment details in the Azure portal and view the overall experiment details before viewing the details for the individual run that produced the best result. There's lots of information here about the performance of the model generated and how its features were used.\n\nLet's get the best run and the model that was generated (you can ignore any warnings about Azure ML package versions that might appear)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = automl_run.get_output()\nprint(best_run)\nprint(fitted_model)\nbest_run_metrics = best_run.get_metrics()\nfor metric_name in best_run_metrics:\n    metric = best_run_metrics[metric_name]\n    print(metric_name, metric)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "One of the options you used was to include model *explainability*. This uses a test dataset to evaluate the importance of each feature. You can view this data in the notebook widget or the portal, and you can also retrieve it from the run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n\nshap_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = retrieve_model_explanation(best_run)\n\n# Overall feature importance (the Feature value is the column index in the training data)\nprint(\"Feature\\tImportance\")\nfor i in range(len(overall_imp)):\n    print(overall_imp[i], '\\t', overall_summary[i])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, having found the best performing model, you can register it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Register model\nbest_run.register_model(model_path='outputs/model.pkl', model_name='diabetes_model', tags={'Training context':'Auto ML'}, properties={'AUC': best_run_metrics['AUC_weighted'], 'Accuracy': best_run_metrics['accuracy']})\n\n# List registered models\nfor model in Model.list(ws):\n    print(model.name, 'version:', model.version)\n    for tag_name in model.tags:\n        tag = model.tags[tag_name]\n        print ('\\t',tag_name, ':', tag)\n    for prop_name in model.properties:\n        prop = model.properties[prop_name]\n        print ('\\t',prop_name, ':', prop)\n    print('\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Now you've seen several ways to leverage the high-scale compute capabilities of the cloud to experiment with model training and find the best performing model for your data. In the next exerise, you'll deploy a registered model into production."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}